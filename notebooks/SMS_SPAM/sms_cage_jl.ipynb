{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **End to end tutorial for SMS spam using SPEAR(Cage and JL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining an Enum to hold labels**\n",
    "### **Representation of class Labels**\n",
    "\n",
    "<p>All the class labels for which we define labeling functions are encoded in enum and utilized in our next tasks. Make sure not to define an Abstain(Labeling function(LF) not deciding anything) class inside this Enum, instead use the Abstain object as used later in LF section.</p>\n",
    "\n",
    "<p>SPAM dataset contains 2 classes i.e <b>HAM</b> and <b>SPAM</b>. Note that the numbers we associate can be anything but it is suggested to use a continuous numbers from 0 to number_of_classes-1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# enum to hold the class labels\n",
    "class ClassLabels(enum.Enum):\n",
    "    SPAM = 1\n",
    "    HAM = 0\n",
    "\n",
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining preprocessors, continuous_scorers, labeling functions**\n",
    "During labeling the unlabelled data we lookup for few keywords to assign a class SMS.\n",
    "\n",
    "<b>Example</b> : *If a message contains apply or buy in it then most probably the message is spam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigWord1 = {\"free\",\"credit\",\"cheap\",\"apply\",\"buy\",\"attention\",\"shop\",\"sex\",\"soon\",\"now\",\"spam\"}\n",
    "trigWord2 = {\"gift\",\"click\",\"new\",\"online\",\"discount\",\"earn\",\"miss\",\"hesitate\",\"exclusive\",\"urgent\"}\n",
    "trigWord3 = {\"cash\",\"refund\",\"insurance\",\"money\",\"guaranteed\",\"save\",\"win\",\"teen\",\"weight\",\"hair\"}\n",
    "notFreeWords = {\"toll\",\"Toll\",\"freely\",\"call\",\"meet\",\"talk\",\"feedback\"}\n",
    "notFreeSubstring = {\"not free\",\"you are\",\"when\",\"wen\"}\n",
    "firstAndSecondPersonWords = {\"I\",\"i\",\"u\",\"you\",\"ur\",\"your\",\"our\",\"we\",\"us\",\"youre\"}\n",
    "thirdPersonWords = {\"He\",\"he\",\"She\",\"she\",\"they\",\"They\",\"Them\",\"them\",\"their\",\"Their\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declaration of a simple preprocessor function**\n",
    "\n",
    "\n",
    "For most of the tasks in NLP, computer vivsion instead of using the raw datapoint we preprocess the datapoint and then label it. Preprocessor functions are used to preprocess an instance before labeling it. We use **`@preprocessor(name,resources)`** decorator to declare a function as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "\n",
    "@preprocessor(name = \"LOWER_CASE\")\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "lower = convert_to_lower(\"RED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some Labeling function(LF) definitions**\n",
    "Below are some examples on how to define LFs and continuous LFs(CLFs). To get the continuous score for a CLF, we need to define a function with continuous_scorer wrapper(just like labeling_function wrapper) and pass it to a CLF as displayed below. Also note how the continuous score can be used in CLF. Note that the word_similarity is the function with continuous_scorer wrapper and is written in con_scorer file(this file is not a part of package) in same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import labeling_function, ABSTAIN\n",
    "\n",
    "from con_scorer import word_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "@preprocessor()\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF1(c,**kwargs):    \n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF2(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF3(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM \n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF4(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF5(c,**kwargs):\n",
    "    for pattern in kwargs[\"keywords\"]:    \n",
    "        if \"free\" in c.split() and re.search(pattern,c, flags= re.I):\n",
    "            return ClassLabels.HAM\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF6(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF7(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(label=ClassLabels.SPAM)\n",
    "def LF8(c,**kwargs):\n",
    "    if (sum(1 for ch in c if ch.isupper()) > 6):\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF1(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF2(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF3(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF4(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF5(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF6(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF7(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=lambda x: 1-np.exp(float(-(sum(1 for ch in x if ch.isupper()))/2)),label=ClassLabels.SPAM)\n",
    "def CLF8(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Accumulating all LFs into rules, an LFset(a class) object**\n",
    "### **Importing LFSet and passing LFs we defined, to that class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import LFSet\n",
    "\n",
    "LFS = [LF1,\n",
    "    LF2,\n",
    "    LF3,\n",
    "    LF4,\n",
    "    LF5,\n",
    "    LF6,\n",
    "    LF7,\n",
    "    LF8,\n",
    "    CLF1,\n",
    "    CLF2,\n",
    "    CLF3,\n",
    "    CLF4,\n",
    "    CLF5,\n",
    "    CLF6,\n",
    "    CLF7,\n",
    "    CLF8\n",
    "      ]\n",
    "\n",
    "rules = LFSet(\"SPAM_LF\")\n",
    "rules.add_lf_list(LFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading data**\n",
    "### **Load the data: X, X_feats, Y**\n",
    "<p>Note that the utils below is not a part of package but is used to load the necessary data. X is the raw data that is to be passed to LFs, X_feats is a numpy array of shape (num_instances, num_features) and Y are true labels(if available).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data_to_numpy, get_various_data\n",
    "\n",
    "X, X_feats, Y = load_data_to_numpy()\n",
    "\n",
    "validation_size = 100\n",
    "test_size = 200\n",
    "L_size = 100\n",
    "U_size = 4000\n",
    "n_lfs = len(rules.get_lfs())\n",
    "\n",
    "X_V, Y_V, X_feats_V,_, X_T, Y_T, X_feats_T,_, X_L, Y_L, X_feats_L,_, X_U, X_feats_U,_ = get_various_data(X, Y,\\\n",
    "    X_feats, n_lfs, validation_size, test_size, L_size, U_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Labeling data**\n",
    "### **Paths**\n",
    "* path_json: path to json file generated by PreLabels\n",
    "* V_path_pkl: path to pkl file generated by PreLabels containing the validation data with true labels\n",
    "* L_path_pkl: path to pkl file generated by PreLabels containing the labeled data with true labels\n",
    "* T_path_pkl: path to pkl file generated by PreLabels containing the test data with true labels\n",
    "* U_path_pkl: path to pkl file generated by PreLabels containing the unlabelled data without true labels\n",
    "* log_path: path to save the log which is generated during the algorithm\n",
    "<p>Difference between test and labeled data is that labeled data may be used in the algorithm(JL uses it while Cage doesn't) but test data isn't. Make sure to have the pickle files <font color='red'><b>EMPTY</b></font> ie, it should not any data inside it before passing to .generate_pickle() member function of PreLabels</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'data_pipeline/sms_json.json'\n",
    "V_path_pkl = 'data_pipeline/sms_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/sms_pickle_T.pkl' #test data - have true labels\n",
    "L_path_pkl = 'data_pipeline/sms_pickle_L.pkl' #Labeled data - have true labels\n",
    "U_path_pkl = 'data_pipeline/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "\n",
    "log_path_cage_1 = 'log/cage_log_1.txt' #cage is an algorithm, can be found below\n",
    "log_path_jl_1 = 'log/jl_log_1.txt' #jl is an algorithm, can be found below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing PreLabels class and using it to label data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.90it/s]\n",
      "100%|██████████| 200/200 [00:11<00:00, 16.79it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.14it/s]\n",
      "100%|██████████| 4000/4000 [04:07<00:00, 16.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import PreLabels\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_V,\n",
    "                               gold_labels=Y_V,\n",
    "                               data_feats=X_feats_V,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(V_path_pkl)\n",
    "sms_noisy_labels.generate_json(path_json) #generating json files once is enough\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_T,\n",
    "                               gold_labels=Y_T,\n",
    "                               data_feats=X_feats_T,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(T_path_pkl)\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_L,\n",
    "                               gold_labels=Y_L,\n",
    "                               data_feats=X_feats_L,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(L_path_pkl)\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_U,\n",
    "                               rules=rules,\n",
    "                               data_feats=X_feats_U,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(U_path_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Accessing labeled data**\n",
    "### **Importing and the use of get_data and get_classes**\n",
    "<p>These functions can be used to extract data from pickle files and json file respectively. Note that these are the files generated using PreLabels.</p>\n",
    "<p>For detailed contents of output, please refer documentation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data list:  10\n",
      "Shape of feature matrix:  (4000, 1024)\n",
      "Shape of labels matrix:  (4000, 16)\n",
      "Shape of continuous scores matrix :  (4000, 16)\n",
      "Total number of classes:  2\n",
      "Classes dictionary in json file(modified to have integer keys):  {1: 'SPAM', 0: 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "data_U = get_data(U_path_pkl, check_shapes=True)\n",
    "#check_shapes being True(above), asserts for relative shapes of arrays in pickle file\n",
    "print(\"Number of elements in data list: \", len(data_U))\n",
    "print(\"Shape of feature matrix: \", data_U[0].shape)\n",
    "print(\"Shape of labels matrix: \", data_U[1].shape)\n",
    "print(\"Shape of continuous scores matrix : \", data_U[6].shape)\n",
    "print(\"Total number of classes: \", data_U[9])\n",
    "\n",
    "classes = get_classes(path_json)\n",
    "print(\"Classes dictionary in json file(modified to have integer keys): \", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cage Algorithm**\n",
    "### **Importing Cage class (the algorithm) and declaring an object of it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.Cage import Cage\n",
    "\n",
    "cage = Cage(path_json, n_lfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use of fit_and_predict_proba function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class. For more details about arguments, please refer documentation; same should be the case for any of the member functions used from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.81\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5250000000000001\n",
      "probs shape:  (4000, 2)\n",
      "labels shape:  (4000,)\n"
     ]
    }
   ],
   "source": [
    "probs = cage.fit_and_predict_proba(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                                   qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01)\n",
    "labels = np.argmax(probs, 1)\n",
    "print(\"probs shape: \", probs.shape)\n",
    "print(\"labels shape: \",labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use of fit_and_predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers, having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.79\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5116279069767442\n",
      "labels shape:  (4000,)\n"
     ]
    }
   ],
   "source": [
    "labels = cage.fit_and_predict(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                              qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01, \\\n",
    "                              need_strings=False)\n",
    "print(\"labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use of fit_and_predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing strings, having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.79\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5116279069767442\n",
      "labels_strings shape:  (4000,)\n"
     ]
    }
   ],
   "source": [
    "labels_strings = cage.fit_and_predict(U_path_pkl, T_path_pkl, log_path_cage_1, need_strings=True)\n",
    "print(\"labels_strings shape: \", labels_strings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save parameters**\n",
    "<p>Make sure the pickle you are passing here is empty</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage.save_params('params/sms_cage_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_2 = Cage(path_json, n_lfs)\n",
    "cage_2.load_params('params/sms_cage_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use of predict_proba function of Cage class**\n",
    "The output(probs_test) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in Cage class. Hope you have loaded parameters.\n",
      "probs_test shape:  (200, 2)\n"
     ]
    }
   ],
   "source": [
    "probs_test = cage_2.predict_proba(path_test = T_path_pkl, qc = 0.85) \n",
    "#NEED NOT use the same test data(above) used in Cage class before.\n",
    "print(\"probs_test shape: \",probs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use of predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(strings) if need_strings is Flase(True), having the classes of each instance. Just the use case with need_strings as False is displayed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in Cage class. Hope you have loaded parameters.\n",
      "labels_test shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "labels_test = cage_2.predict(path_test = T_path_pkl, qc = 0.85, need_strings = False)\n",
    "print(\"labels_test shape: \", labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Joint Learning(JL) Algorithm**\n",
    "### **Subset selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
